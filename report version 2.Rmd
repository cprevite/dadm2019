--- 
title: 'Report: Data Analytics for Decision Making'
author: "Claudio Previte and Ana -Maria Casian"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: TRUE
    toc_float:
      collapsed: true
      smooth_scroll: true
    fig_caption: yes
    code_folding: hide 
---

<style>
body {
text-align: justify}
</style> 


```{r, message=FALSE, warning=FALSE}
library(here)
library(Hmisc)
library(dplyr)
library(DataExplorer)
library(ggplot2)
library(lattice)
library(inspectdf)
library(kableExtra)
library(explore)
library(DT)
library(dplyr)
library(Boruta)
library(corrr)
library(tidyverse)
library(caret)
library(randomForest)
library(funModeling)
library(gmodels)
library(data.table)
library(DMwR)

```

# Introduction

Data science is gaining more ground in this day and age and decision making based on machine learning algorithms is a big part of it. It intends to assist the user in taking fast and smart decision in an era where time management stands as a competitive advantage and minimising risks, monetary or reputational, are the pinnacle for the success. Would a machine learning algorithm be able to predict the RTD Tea drinks consumption in Spain? What are the factors that are driving the consumption? Are they the same in the neighbourhood country? This report will
answer these questions and touch upon new discussions. The paper is built in two parts. The first one is an introduction to the background of the topic, an overview of the data sources and of the data manipulation. The second part consists of the analysis of the data, the statistical models used and the final discussion

- purpose of the course
- the benefit of Machine Learning in general



## Case description

- describe the problem
- benefit of Machine Learning in this special case


Quality issues related to the final products have been dected within the printing company.  During the print run process, grooves are sometimes formed in the cylinder, which leads to a product with defects, that cannot be provided to the customer. Moreover, when this type of quality related problem is detected, the run is suspended and the cylinder has to be removed and fixed before using it again in production. This event  disrupts the manufacturing process thus raising the risk of delaying the delivery date of the costumer order, but also translats im the fact that the efficiency of the whole operation is slowed down by turning off production, removing the cylinder and replacing it. These factors in turn decrease profits, as the machine is not producing as many final products as it should be and the employees are payed for idle time. Moreover, the costs increase as repair costs occur.

Given the situation described again, the company has decided to investigate why the defects are generated during the runs. For this, the firm has gather data on 512 observations. The atttributes that describe these data points are mounting 40, out of which 20 attributes are numeric and 20 are nominal.The variable of interest is represented by the class band type, which in turn is formed by 312 instances of "no band" and 200 instances of "band".


```{r,  echo = FALSE, warning = FALSE, message = FALSE, out.width = "700px"}

knitr::include_graphics("C:/Users/claud/Desktop/Ana/HEC/3rd semester/Projects in Data Analytics/project/dadm2019/pictures/attributes1.PNG")

```

```{r,  echo = FALSE, warning = FALSE, message = FALSE, out.width = "700px"}

knitr::include_graphics("C:/Users/claud/Desktop/Ana/HEC/3rd semester/Projects in Data Analytics/project/dadm2019/pictures/attributes2.PNG")

```

## Analysis

- classification model tested
- which ones are more adapted to the problem

## Results

- show results, ac-curacies
- recommendations and implementations



<!-- ## Source of data -->

<!-- - where the data comes from -->
<!-- - quick description of the data -->


<!-- # Data processing -->

<!-- - explain the purpose of data cleaning -->
<!-- - why it is so important in data analytic -->


## Loading data


The data has  been provided in a csv format. As stated before, the file contrains 512 ponservations. The imported file has been saved in a variable named "bands". The next part focuses on data description, in order to gather and understand as much insights about the variables as possible. This is a very important step before plinging into  the model creation, as it allows the user to familariaze itself with the data set, 
```{r}
bands <- read.csv2(file = here('bands3.csv'), sep = ';',na.strings = "?")
```


## Data visualization

The first task to analyze a dataset is to inspect all the variables and their values. In order to do that, the 'str' was applied and the results are showed here below.

```{r str, echo=FALSE, fig.cap= 'Figure 1', fig.align='center'}
str(bands)
```

It can be noticed that during import, the sytsem classified some numeric variables as factors. In this respect, a transformation  will be performed in order to correct the missclassification. Furthermore, the data are not recorded in a consistent way for all the observations: part of inputs  are in upper cases and some are in lower cases. This can occur when  different people are in charge to record the data or it could be related to the different practices among  the locations from where the data comes from. This problem can lead to a bad data perception, so it has been decided to trasnform all categorical data to upper cases. 

In addition. it has been observed that some nominal variables are characterized by  a high number of levels, which means that they will not be really useful for a potential classification task. At this stage, no transformation will be performed regarding this matter, but the subject will be rediscssed further in the report.

At the same time, variables with only one level don't bring any information for the classification, so they will be removed from the dataset. 

It has already become clear at this stage that this dataset needs to be cleaned before analysing it.


### Standardization of the dataset

The first transformation is realated to the misclassification mentioned above (numeric variables classified as factors) and the transformation of the categorical variables to uppcarecase, so that the data is consistent.

```{r standardization, message=FALSE, warning=FALSE, include=FALSE}


#change class to numeric for numeric variables 
cols_numeric = c(21:39)  
bands[,cols_numeric] = apply(bands[,cols_numeric], 2, function(x) as.numeric(as.character(x))) 

# uppercase for all the factor values
bands <- as.data.frame(lapply(bands,function(x) 
  if(is.factor(x)) factor(toupper(x)) 
  else(x)))


```

Here below is an overview of the dataset.

```{r datatable, echo=FALSE, message=FALSE, warning=FALSE, fig.cap= 'Figure 2: Datatable', fig.align='center'}
kable(bands, 'html') %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
   scroll_box(width = "100%", height = "400px")
```

## Missing values

The next step is having a closer look at the missing variables. First the 'plot_intro'  and 'plot_na' functions were used to have a global viw of the status of the variables.


```{r echo=FALSE, message=FALSE, warning=FALSE, fig.cap= 'Figure 3',fig.align='center'}
# bands %>% 
#   introduce() %>%
#   t()%>%
#   kable() %>%
#   kable_styling(bootstrap_options = c("striped", "hover","condensed","bordered"),full_width = F, position = "center")

plot_intro(bands)

```

It can be observed  from Figure 3, that 4.6 % of the data is missing and the complete rows(i.e. an observation containing a value for all the variables)  represents only 51 % of the total observations.

```{r percentage na, fig.cap= 'Figure 4: Prevalence of NAs', fig.align='center'}

x<-inspect_na(bands)
show_plot(x, col_palette=2)
```
Figure 4 shows that all the 'NAs' of the bands dataset come from a total of  28 variables, with a special mention for the **location** variables which counts 156 missing values (almost 30 % percent of the observations are missing). Then, a total of  10 variables have the same amount of missing values (an average of 57) that account for 10% of the observatios in each attribute. The repeating patterns drives to a further analysis of the missing values, which was performed in the falowing part.

### Structure of the missing values

As stated before, to investigate the findings mentioned above related to the identical pourcentage of missing values, another  plot  was proposed to have a better look at the distribution of missing values. Having said that, the  graph below represents the missing values per variable per row. 

```{r structure of NAs, echo=FALSE, message=FALSE, warning=FALSE, fig.cap= 'Figure 5: Missing values in rows', fig.align='center'}


row.plot <- bands %>%
  mutate(id = row_number()) %>%
  gather(-id, key = "key", value = "val") %>%
  mutate(isna = is.na(val)) %>%
  ggplot(aes(key, id, fill = isna)) +
    geom_raster(alpha=0.8) +
    scale_fill_manual(name = "",
        values = c('steelblue', 'tomato2'),
        labels = c("Present", "Missing")) +
    labs(x = "Variable",
           y = "Row Number", title = "Missing values in rows") +
    coord_flip()

row.plot
```

Figure 5 is confirming the intuition preveiously stated. The repartition of missing values seems to be concentrated in the very last observations for several variables. This indicates that chuncks of consecutive values are missing. This could be due to a problem in recording the observations, or more simply due to the fact that from a certain point in time, they decided  not to record these specific variables anymore. 

For a total of 13 variables, information is missing starting the 486th observation, which means that information is lost on the respective variables. This raises the question of weather it is useful to keep these observations for the analysis; deleting them would reduce the dataset of 54 rows.

Keeping them implies having to replace the missing values for 13 variables by an imputation method (knn, mean, median...). Regardless of the method that is chosen for replacing the absent values, this assumes that 54 consecutive observation will share the same value , thus having 10% of the variable sharing the same savlue. It is stronly believed that this is going to biase the results. Based on this point, it has been decided to delete the last 54 consecutive observations of the dataset, which results in having a  dataset with 485 observations.

```{r delete last obs, message=FALSE, warning=FALSE}
#Delete the last rows that have consecutive missing data
bands1 <- bands[-c(486:540),]
```


At this point, a test has been carried out to verify the 'NAs' state.

```{r percentage na 2, fig.cap= 'Figure 4: Prevalence of NAs'}

x1<-inspect_na(bands1)
show_plot(x1, col_palette=2)

bands1 %>%
  introduce() %>%
  t() %>%
  kable() %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed", "bordered"),
    full_width = F,
    position = "center") 
```

It can be observed that the variable **location** is the only one to still have an important ratio of missing values (more than 20%). Replacing all these values with an imputation method doen't seem to be the best option, as the proportion of missing is too high out of the total observations. An alternative has been  decided:  to set these NAs as **UNDEFINED**, this adding a 7th level to the variable. 


```{r location}
# replace missing values of "location variable" with value "Undefined"
bands1$location = as.character(bands1$location)
bands1$location[is.na(bands1$location)] <- "UNDEFINED"
bands1$location = as.factor(bands1$location)

```


### Imputation of the missing values

The amount of missing values for the rest of the variables is perceived as acceptable and these variables will be imputed by a **k-Nearest-Neighbors** (kNN) method. KNN is an algorithm that is useful for matching a point with its closest k neighbors in a multi-dimensional space. It can be used for data that are continuous, discrete, ordinal and categorical which makes it particularly useful for dealing with all kind of missing data. We have to set a value for **k** and decide to go for the simple approach to set k = sqrt(n) = 22. 

```{r}
# replace missing values with knn 


n <- nrow(bands1)
n
k <- sqrt(n)
k

bands1 <- knnImputation(bands1, k=22)

# check missing value
anyNA(bands)
```

## Selection of the variables

### First elimination


As the database is complete and has no more missing values, the variable are examined again, only that this time from a different  perspective. In fact, there are a lot of attributes that are useless to define if the product presents defects or not. First of all,  variables with only 1 level of factor,such as  **ink_color** and **cylinder_division** can be removed as they won't have any effect on the response variable. Moreover, the variable **job_number** has too many levels (more than 200) and no link with  the response variable. The variables **customer** and **date** can also be deleted as it have no possibiliies to affect the class of 'band_type'. 

In the case of the variable **cylinder_no**,  further analysis is require. The number of levels indicated that this variable is better to be deleted, because it has almost the same number of levels (429) as observations (485) and it doesn't make sense to keep it to define the class 'band_type'. Nevertheless, the cylinder is the most important part of the printing processus, as when a problem of banding is detected in the processus, the cylinder has to be removed and repaired. So that means that it has a direct effect on the response variable. Having a closer look at the variable, it has been noticed that the values for this variable were falowing a pattern :  "LETTER NUMBER NUMBER NUMBER". Thus, the letter could correspond to a batch number or type of cylinder or the location where it has been produced, which actually could explain the variable of interest. It has been decided to keep only the letter out of the whole string, for each value of this variable. Doing so the number of levels drops to 17, which is acceptable in terms of the analysis. 

This first attempt to reduce the number of attribute results in a database with 35 variables, which seems still high for a good analysis. Another method based on statistical features will be used in order to select the most important attributes to keep for the analysis. 

```{r delete var}
# Delete variables that are not needed
bands1 <-
  bands1[, which(
    !names(bands1) %in% c(
      "date", # useless
      "ink_color", # only 1 level
      "customer", # don't depend on the printing processus
      'cylinder_division', # only 1 level
      'job_number' # too many levels
    )
  )]

bands1$cylinder_no <- as.factor(substr(bands1$cylinder_no, 0, 1))
```




### Boruta test - Second elimination


The Boruta algorithm is a wrapper built around the random forest classification algorithm. It tries to capture all the important and interesting features we might have in our dataset with respect to an outcome variable.
We can explain how the algorithm works with help of this [website](https://www.r-bloggers.com/feature-selection-with-the-boruta-algorithm/).

Let’s assume we have a target vector T (the response variable **band_type**) and a bunch of predictors P (all the other variables).

The Boruta algorithm starts by duplicating every variable in P, but instead of making a row-for-row copy, it permutes the order of the values in each column. So, in the copied columns (let’s call them P’), there should be no relationship between the values and the target vector.

Boruta then trains a Random Forest to predict T based on P and P’.

The algorithm then compares the variable importance scores for each variable in P with it’s “shadow” in P’. If the distribution of variable importances is significantly greater in P than it is in P’, then the Boruta algorithm considers that variable significant. 


```{r message=FALSE, warning=FALSE, include=FALSE}


boruta_test <- Boruta(band_type ~ ., data = bands1, doTrace = 2)
table_result <- as.data.frame(boruta_test$finalDecision)
setDT(table_result, keep.rownames = TRUE)[]
colnames(table_result) <- c("Variable", "Decision")

kable(table_result) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed", "bordered"),
    full_width = F,
    position = "center"
  ) %>%
  footnote(symbol = strwrap("Results of the boruta test")) %>%
  scroll_box(width = "500px", height = "300px")


variables_to_keep <- table_result %>%
  filter(table_result$Decision == 'Confirmed' |
           table_result$Decision == 'Tentative')

band_typeVar <- data.frame('band_type', 'Confirmed')
names(band_typeVar) <- c('Variable', 'Decision')

variables_to_keep <- rbind(variables_to_keep, band_typeVar)

names_variables_to_keep <- variables_to_keep$Variable

names_variables_to_keep

bands1 <-
  bands1[, which(
    names(bands1) %in% names_variables_to_keep
  )]

```



```{r plot boruta, echo=FALSE, fig.align='center', fig.cap='Importance of variable with Boruta'}
plot(boruta_test, xlab = "", xaxt = "n")
lz <- lapply(1:ncol(boruta_test$ImpHistory), function(i)
  boruta_test$ImpHistory[is.finite(boruta_test$ImpHistory[, i]), i])
names(lz) <- colnames(boruta_test$ImpHistory)
Labels <- sort(sapply(lz, median))
axis(
  side = 1,
  las = 2,
  labels = names(Labels),
  at = 1:ncol(boruta_test$ImpHistory),
  cex.axis = 0.8
)
```

The results of the Boruta algorithm are shown in this plot, with the importance of each variables and the final decision to keep it or not. The first thing we observe is that there are a lot of important variables but some are more than others, specially the right hand side of the elbow. Then we notice that the in the 12 most important variables, only one (**grain_screened**) is impacted by the problem of the last 54 observations presenting only missing values. So, we decide to take in consideration an other database and test both in parallel. 


1) the 485 first observations with all the variables selected by Boruta

2) the 540 observations with the 11 most important variables selected by Boruta (excluding **grain_screened**)

```{r}
bands2 <- knnImputation(bands, k=22)

bands2 <-
  bands2[, which(
    names(bands2) %in% c(
      "press_type", # useless
      "press", # only 1 level
      "press_speed", # don't depend on the printing processus
      'ink_type', # only 1 level
      'viscosity',
      'cylinder_no',
      'hardener',
      'cylinder_size',
      'humidity',
      'roller_durometer',
      'cylinder_type',
      'ink_pct',
      'band_type'
    )
  )]

bands2$cylinder_no <- as.factor(substr(bands2$cylinder_no, 0, 1))
```


```{r confirmed variable, echo=FALSE, fig.align='center', fig.cap= 'Table of the final decision of Boruta'}
kable(table_result) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed", "bordered"),
    full_width = F,
    position = "center"
  ) %>%
  footnote(symbol = strwrap("Results of the boruta test")) %>%
  scroll_box(width = "500px", height = "300px")
```

This table presents more clearly the variables we will keep.

## Outliers

We have generated the boxplot and the summary for all the variables and examinated the outliers. Given that the outliers were not showing any significant values, we have decided that there are no error meassurements and neither error entries in the data.

```{r, message=FALSE, warning=FALSE, include= FALSE}

numeric_column <- names(select_if(bands, is.numeric))

for (i in numeric_column) {
  print(summary(bands[i]))
  print (boxplot(bands[i])$out)
}

```


# Exploratory Data Analysis

- find an order for the different plot that make sense
- do not put a plot if it is not useful
- get insights

```{r fig.height= 12}


plot_histogram(bands)

plot_density(bands)



plot_boxplot(bands, by= 'band_type',  ncol = 2, title = "Side-by-side boxplots")

# split data in 2
bands.band <- filter(bands, bands$band_type == 'BAND')

bands.noband <- filter(bands, bands$band_type == 'NOBAND')


```

```{r, fig.width = 10, fig.height = 20}


bands%>%
  explore_all(target = band_type)
```




