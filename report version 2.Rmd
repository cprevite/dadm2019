--- 
title: 'Report: Text mining'
author: "Claudio Previte and Ana -Maria Casian"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: TRUE
    toc_float:
      collapsed: true
      smooth_scroll: true
---



```{r message=FALSE, warning=FALSE}
library(here)
library(Hmisc)
library(dplyr)
library(DataExplorer)
library(ggplot2)
library(lattice)
library(inspectdf)
library(kableExtra)
library(explore)
library(DT)
library(dplyr)
library(Boruta)
library(corrr)
library(tidyverse)
library(caret)
library(randomForest)
library(funModeling)
library(gmodels)

```



# Executive summary

## Challenges

## Analysis

## Results

# General

## Source of data

## Case description

# Data processing


## Loading data

```{r}
bands <- read.csv2(file = here('bands3.csv'), sep = ';',na.strings = "?")
```


## Data visualisation

```{r}
str(bands)
```


### First data transformation to continue looking at the vars

```{r }


#change class to numeric for numeric variables 
cols_numeric = c(21:39)  
bands[,cols_numeric] = apply(bands[,cols_numeric], 2, function(x) as.numeric(as.character(x))) 

# uppercase for all the factor values
bands <- as.data.frame(lapply(bands,function(x) 
  if(is.factor(x)) factor(toupper(x)) 
  else(x)))


```




```{r}
datatable(bands)

bands %>% 
  introduce() %>%
  t()%>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover","condensed","bordered"),full_width = F, position = "center")%>%
  footnote(symbol = strwrap("Sort description of the data"))


plot_intro(bands)

summary(bands)

bands %>% 
  explore::describe() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover","condensed","bordered"),full_width = F, position = "center")

show_plot(inspect_na(bands), col_palette=2)

```


### Check the data structure

```{r, message=FALSE, warning=FALSE, include= FALSE}
str(bands) #this part will not apear in the report, but we should comment about it. This line of code helped us to decide wich data to transform in the first part

```

### Check the structure of the missing values

In this part we use the'aggr' function Calculate or plot the amount of missing/imputed values in each variable and the amount of missing/imputed values in certain combinations of variables.
```{r}

library(VIM)
aggr(bands[, which(sapply(bands, function(x)all(any(is.na(x)))) == TRUE)], sortVar=TRUE, cex.lab=0.8, cex.axis=0.9,  numbers = TRUE, combined = TRUE,  oma = c(8,1,1,1))


```



```{r}


row.plot <- bands %>%
  mutate(id = row_number()) %>%
  gather(-id, key = "key", value = "val") %>%
  mutate(isna = is.na(val)) %>%
  ggplot(aes(key, id, fill = isna)) +
    geom_raster(alpha=0.8) +
    scale_fill_manual(name = "",
        values = c('steelblue', 'tomato3'),
        labels = c("Present", "Missing")) +
    labs(x = "Variable",
           y = "Row Number", title = "Missing values in rows") +
    coord_flip()

row.plot
```

EDA
```{r}
bands %>%
  introduce() %>%
  t() %>%
  kable() %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed", "bordered"),
    full_width = F,
    position = "center"
  ) %>%
  footnote(symbol = strwrap("Short description of the data"))


plot_intro(bands)
```





```{r}
#Delete the last rows that have consecutive missing data
bands <- bands[-c(486:540),]

# replace missing values of "location variable" with value "Undefined"
bands$location = as.character(bands$location)
bands$location[is.na(bands$location)] <- "UNDEFINED"
bands$location = as.factor(bands$location)

# Delete variables that are not needed
bands <-
  bands[, which(
    !names(bands) %in% c(
      "date",
      "ink_color",
      "customer", 
      'cylinder_division',
      'cylinder_no',
      'job_number'
      
    )
  )]
```

```{r}
# replace missing values with knn 
# we have to decide if we want to reÃ¨lace only factor or all variables with this method...
library(DMwR)

bands <- knnImputation(bands, k=10)

# check missing value
anyNA(bands)
```




```{r}


boruta_test <- Boruta(band_type ~ ., data = bands, doTrace = 2)
table_result <- as.data.frame(boruta_test$finalDecision)
setDT(table_result, keep.rownames = TRUE)[]
colnames(table_result) <- c("Variable", "Decision")

kable(table_result) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed", "bordered"),
    full_width = F,
    position = "center"
  ) %>%
  footnote(symbol = strwrap("Results of the boruta test")) %>%
  scroll_box(width = "500px", height = "300px")


plot(boruta_test, xlab = "", xaxt = "n")
lz <- lapply(1:ncol(boruta_test$ImpHistory), function(i)
  boruta_test$ImpHistory[is.finite(boruta_test$ImpHistory[, i]), i])
names(lz) <- colnames(boruta_test$ImpHistory)
Labels <- sort(sapply(lz, median))
axis(
  side = 1,
  las = 2,
  labels = names(Labels),
  at = 1:ncol(boruta_test$ImpHistory),
  cex.axis = 0.8
)


variables_to_keep <- table_result %>%
  filter(table_result$Decision == 'Confirmed' |
           table_result$Decision == 'Tentative')

band_typeVar <- data.frame('band_type', 'Confirmed')
names(band_typeVar) <- c('Variable', 'Decision')

variables_to_keep <- rbind(variables_to_keep, band_typeVar)

names_variables_to_keep <- variables_to_keep$Variable

names_variables_to_keep

```


# Deleting useless variables


```{r}

bands <-
  bands[, which(
    names(bands) %in% names_variables_to_keep
  )]

```



# Outliers. We have generated the boxplot and the summary for all the variables and examinated the outliers. Given that the outliers were not showing any significant values, we have decided that there are no error meassurements and neither error entries in the data

```{r, message=FALSE, warning=FALSE, include= FALSE}

numeric_column <- names(select_if(bands, is.numeric))

for (i in numeric_column) {
  print(summary(bands[i]))
  print (boxplot(bands[i])$out)
}

```



```{r fig.height= 12}


plot_histogram(bands)

plot_density(bands)



plot_boxplot(bands, by= 'band_type',  ncol = 2, title = "Side-by-side boxplots")

plot_correlation(bands, type= 'c', cor_args = list( 'use' = 'complete.obs'))


numeric_column <- names(select_if(bands, is.numeric))
bands %>% 
  select(numeric_column, band_type) %>%
  explore_all(target = band_type)

#ggpairs(bands[,-40], ggplot2::aes(colour=band_type))

# split data in 2
bands.band <- filter(bands, bands$band_type == 'BAND')

bands.noband <- filter(bands, bands$band_type == 'NOBAND')


```

```{r, fig.width = 10, fig.height = 20}

bands%>%
  explore_all(target = band_type)
```







## MODELS

```{r}
#normalize data
# doesn't change the results so we will not keep the modifications
# b.process <- preProcess(bands, method= 'range')
# b <- predict(b.process, newdata = bands)
```


# test and train sets
```{r}

# Create the training and test datasets
set.seed(100)

# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(bands$band_type, p=0.75, list=FALSE)

# Step 2: Create the training  dataset
trainData <- bands[trainRowNumbers,]

# Step 3: Create the test dataset
testData <- bands[-trainRowNumbers,]
```



```{r}
# MARS Multivariate Adaptive Regression Splines
# recursive feature elimination (RFE)
set.seed(100)
options(warn = -1)

subsets <- c(1:6, 10, 15, 35)

ctrl <- rfeControl(
  functions = rfFuncs,
  method = "repeatedcv",
  repeats = 5,
  verbose = FALSE
)

lmProfile <- rfe(
  x = trainDataMars,
  y = trainDataMars$band_type,
  sizes = subsets,
  rfeControl = ctrl
)

lmProfile


# Set the seed for reproducibility
set.seed(100)

# Train the model using randomForest and predict on the training data itself.
model_mars = train(band_type ~ ., data = trainDataMars, method = 'earth')
fitted <- predict(model_mars)
varimp_mars <- varImp(model_mars)
plot(varimp_mars, main = "Variable Importance with MARS")


model_mars

predicted <- predict(model_mars, testDataMars)
predicted

confusionMatrix(
  reference = testDataMars$band_type,
  data = predicted,
  mode = 'everything',
  positive = 'NOBAND'
)
```

```{r}
# random forest


# Call the function:

integ_mod_1 <- data_integrity_model(data = bands, model_name = "randomForest")
integ_mod_1$data_ok


# Any errors ?
integ_mod_1

model_rf <- randomForest(formula(band_type ~.), 
                          data= trainData, 
                          ntree=500, mtry=4, 
                          importance=TRUE, 
                          localImp=TRUE,
                          na.action=na.roughfix,
                          replace=FALSE)
print(model_rf)

head(round(importance(model_rf), 2))
varImpPlot(model_rf)

rf.pred<-predict(model_rf, testData, type="class")
rf.pred

table(true=testData$band_type, pred = rf.pred)

CrossTable(x=testData$band_type, y=rf.pred, prop.chisq=FALSE)

confusionMatrix(rf.pred, testData$band_type, positive = 'NOBAND')
```




```{r}
# logistic regression

fitControl <- trainControl(method = "none")

model_lr <-
  train(band_type ~ .,
        data = trainData,
        method = "glm",
        trControl = fitControl)

print(model_lr$finalModel)

pred <- predict(model_lr, newdata = testData)

print(table(pred))

mat <-
  confusionMatrix(data = pred,
                  reference = testData$band_type,
                  positive = "NOBAND")
print(mat)

# the results are very poor, so we will focus on other model, because this one would be difficult to improve

```

```{r}
# SVM1

fitControl <- trainControl(method = "cv", number = 5)

model_svm <- train(
  band_type ~ .,
  data =
    trainData,
  method = "svmLinear",
  trControl = fitControl,
  tuneGrid = data.frame(C = 10)
)
print(model_svm)

varimp_svm <- varImp(model_svm)
varimp_svm
plot(varimp_svm, main = "Variable Importance with SVM")


print(confusionMatrix(
  data = predict(model_svm, newdata =
                   testData),
  reference = testData$band_type,
  positive = "NOBAND"
))




```

```{r}
#SVM2


model_svm2 <- train(band_type ~ .,
                          data = trainData, 
                          method = "svmRadialCost",
                          preProcess = "range",
                          trace = FALSE,
                          trControl = trainControl(method = "repeatedcv", number = 10, repeats = 10, 
                                                  verboseIter = FALSE))
model_svm2

plot(model_svm2)

print(confusionMatrix(
  data = predict(model_svm2, newdata =
                   testData),
  reference = testData$band_type,
  positive = "NOBAND"
))



# # this one takes a lot of time
# C <- c(0.25, 0.1, 0.5, 1)
# sigma <- c(0.01, 0.1, 1)
# gr.radial<-expand.grid(C = C, sigma = sigma)
# 
# model_svm3<- train(band_type ~ .,
#                           data = trainData,
#                           method = "svmRadial",
#                           preProcess = "range",
#                           trace=FALSE,
#                           trControl = trainControl(method = "repeatedcv", number = 10, repeats = 5, 
#                                                   verboseIter = FALSE),
#                           tuneGrid=gr.radial)
# 
# 
# model_svm3
# 
# plot(model_svm3)
# 
# print(confusionMatrix(
#   data = predict(model_svm3, newdata =
#                    testData),
#   reference = testData$band_type,
#   positive = "NOBAND"
# ))
```


```{r neural network model}

# numFolds <-
#   trainControl(
#     method = 'cv',
#     number = 1,
#     classProbs = TRUE,
#     verboseIter = TRUE,
#     summaryFunction = twoClassSummary,
#     preProcOptions = list(
#       thresh = 0.75,
#       ICAcomp = 3,
#       k = 5
#     )
#   )
# fit2 <-
#   train(
#     band_type ~ . - band_type,
#     data = trainData,
#     method = 'nnet',
#     preProcess = c('center', 'scale'),
#     trControl = numFolds,
#     tuneGrid = expand.grid(size = c(10), decay = c(0.1))
#   )




model_nnet <- caret::train(band_type ~ ., data = trainData, method = "nnet", preProcess = "range", 
    trace = FALSE, trControl = trainControl(method = "cv"))

model_nnet

varimp_nnet <- varImp(model_nnet)
varimp_nnet
plot(varimp_nnet, main = "Variable Importance with Nnet")

results1 <- predict(model_nnet, newdata = testData)
results1

conf1 <-
  confusionMatrix(results1, testData$band_type, positive = 'NOBAND')
conf1



```

```{r}
# cylinder_no with only the first letter --> no significant results
#bands$cylinder_no <- substr(bands$cylinder_no, 0, 1)

library(rpart)
# Build the model
set.seed(123)
model_cart <- rpart(band_type ~., data = bands, method = "class")  
model_cart
summary(model_cart)

# pruning
par(pty = "s")
with(model_cart, plot(cptable[, 3], xlab = "Tree Number", ylab = "Resubstitution Error (R)", 
    type = "b"))

# Plot the trees
rpart.plot(model_cart, box.palette = "RdBu", nn = FALSE)

printcp(model_cart)



cart_pred <- predict(model_cart, type = 'class')
table(cart_pred, bands$band_type)
cm_cart <- confusionMatrix(cart_pred,bands$band_type, positive = 'NOBAND')
cm_cart
predict_cart <- predict(model_cart, newdata = testData)


#pruning
cart_prune <- prune(model_cart, cp = 0.01)


```

```{r}

# linear discriminant analysis

trainDataLda <- trainData[, -2]
testDataLda <- testData[, -2]

# Estimate preprocessing parameters
preproc.param <- trainDataLda %>% 
  preProcess(method = c("center", "scale"))
# Transform the data using the estimated parameters
train.transformed <- preproc.param %>% predict(trainDataLda)
test.transformed <- preproc.param %>% predict(testDataLda)



library(MASS)
# Fit the model
model_lda <- lda(band_type ~., data = train.transformed)
# Make predictions
predictions <- model_lda %>% predict(test.transformed)
# Model accuracy
mean(predictions$class==test.transformed$band_type)

model_lda
plot(model_lda)

# lda.data <- cbind(train.transformed, predict(model_lda)$x)
# ggplot(lda.data, aes(LD1, LD2)) +
#   geom_point(aes(color = band_type))
```











