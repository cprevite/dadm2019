--- 
title: 'Report: Data Analytics for Decision Making'
author: "Claudio Previte and Ana -Maria Casian"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  rmdformats::readthedown:
    highlight: tango
    code_folding: hide 
---

<style>
body {
text-align: justify}
</style> 



Here is the list of libraries that have been used throughout the project.
```{r, message=FALSE, warning=FALSE}
library(here)
library(Hmisc)
library(dplyr)
library(DataExplorer)
library(ggplot2)
library(lattice)
library(inspectdf)
library(kableExtra)
library(explore)
library(DT)
library(dplyr)
library(Boruta)
library(corrr)
library(tidyverse)
library(caret)
library(randomForest)
library(funModeling)
library(gmodels)
library(data.table)
library(DMwR)

```

# Introduction

Data science is gaining more ground in this day and age and decision making based on machine learning algorithms is a big part of it. It intends to assist the user in taking fast and smart decision in an era where time management stands as a competitive advantage and minimizing risks, monetary or reputational, are the pinnacle for the success. Would a machine learning algorithm be able to predict which product is more likely to have a defect? What are the factors that are contributing to having a defect product?  In this report we are investigating the questions here above as we will try to solve the problem of a printing company that is facing issues with the quality of their products. The paper is built in two parts. The first one is an introduction to the background of study case, an overview of the data sources, data cleaning and of the data manipulation. The second part consists of the analysis of the data, the statistical models used and the final discussion. 

## Case description

Quality issues related to the final products have been detected within the printing company X.  During the print run process, grooves are sometimes formed in the cylinder, which leads to a product with defects, that cannot be provided to the customer. Moreover, when this type of quality related problem is detected, the run is suspended and the cylinder has to be removed and fixed before using it again in production. This event  disrupts the manufacturing process thus raising the risk of delaying the delivery date of the costumer order, but also translates in the fact that the efficiency of the whole operation is slowed down by turning off production, removing the cylinder and replacing it. These factors in turn decrease profits, as the machine is not producing as many final products as it should be and the employees are payed for idle time. Moreover, the costs increase as repairing costs occur.

Given the situation described again, the company has decided to investigate why the defects are generated during the runs. For this, the firm has gather data on 540 observations. The attributes that describe these data points are 40 in total, out of which 24 attributes are numeric and 16 are nominal.The variable of interest is represented by the class 'band type', which in turn is formed by 312 instances of "no band" and 200 instances of "band". 


## Loading data


The data has  been provided in a csv format. As stated before, the table contrains 540 observations. The imported file has been saved in a variable named "bands". The next part focuses on data description, in order to gather and understand as much insights about the variables as possible. This is a very important step before plunging into  the model creation, as it allows the user to familiarize itself with the data set, and thus be able to propose the appropriate models and also to make judgment calls on the results of the models.


```{r}
bands <- read.csv2(file = here('bands3.csv'), sep = ';',na.strings = "?")
```


## Data visualization

The first task that has been undertaken was to inspect all the variables and their values. In order to achieve that, the 'str' function was applied and the results are showed here below.

```{r str, echo=FALSE}
str(bands)

```

It can be noticed that during import, the system classified some numeric variables as factors. In this respect, a transformation  will be performed in order to correct the miss-classification. Furthermore, the data are not recorded in a consistent way for all the observations: part of inputs  are in upper cases and some are in lower cases. This can occur when  different people are in charge to record the data or it could be related to the different practices along  the locations from where the data comes from. This problem can lead to a bad data perception, so it has been decided to transform all categorical data to upper cases. 

In addition. it has been observed that some nominal variables are characterized by  a high number of levels, which means that they will not be really useful for a potential classification task. At this stage, no transformation will be performed regarding this matter, but the subject will be re-discussed further in the report.

At the same time, variables with only one level don't bring any information for the classification, so they will be removed from the dataset. 

It has already become clear at this stage that this dataset needs to be cleaned before moving into the next stage: analysis.


### Standardization of the dataset

The first transformation is related to the miss-classification mentioned above (numeric variables classified as factors) and the transformation of the categorical variables to uppercase, so that the data is consistent.

```{r standardization, message=FALSE, warning=FALSE, include=FALSE}


#change class to numeric for numeric variables 
cols_numeric = c(21:39)  
bands[,cols_numeric] = apply(bands[,cols_numeric], 2, function(x) as.numeric(as.character(x))) 

# uppercase for all the factor values
bands <- as.data.frame(lapply(bands,function(x) 
  if(is.factor(x)) factor(toupper(x)) 
  else(x)))



```

Here below is an overview of the dataset. The first table shows a summary of the data base, and the second table alows the user to explore the whole table. 


```{r summary_kable, echo=FALSE, message=FALSE, warning=FALSE}

bands %>% 
  introduce() %>%
  t()%>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover","condensed","bordered"),full_width = F, position = "left")%>%
  footnote(symbol = strwrap("Sort description of the data"))


```

```{r datatable, echo=FALSE, message=FALSE, warning=FALSE, fig.cap= 'Figure 2b: Datatable', fig.align='center'}
kable(bands, 'html') %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
   scroll_box(width = "100%", height = "400px")
```



## Missing values

The next step is having a closer look at the missing variables. First the 'plot_intro'  and 'plot_na' functions were used to have a global view of the status of the variables.


```{r echo=FALSE, message=FALSE, warning=FALSE, fig.cap= 'Figure 1',fig.align='center', fig.height=c(3), fig.width=c(10)}
plot_intro(bands)

```

It can be observed  from Figure 1, that 4.6 % of the data is missing and the complete rows(i.e. an observation containing a value for all the variables)  represents only 51 % of the total observations.

```{r percentage na, fig.cap= 'Figure 2: Prevalence of NAs', fig.align='center', fig.height=c(4), fig.width=c(10)}

x<-inspect_na(bands)
show_plot(x, col_palette=2)
```
Figure 2 shows that all the 'NAs' of the bands dataset come from a total of  28 variables, with a special mention for the **location** variable, which counts 156 missing values (almost 30 % percent of the observations are missing). Furthermore, a total of 10 variables have the same amount of missing values (an average of 57) that account for 10% of the observations in each attribute. The repeating patterns drives to a further analysis of the missing values, which was performed in the fallowing part.

### Structure of the missing values

As stated before, to investigate the findings mentioned above related to the identical percentage of missing values, another  plot  was proposed to have a better look at the distribution of missing values. Having said that, the  graph below represents the missing values per variable per row. 

```{r structure of NAs, echo=FALSE, message=FALSE, warning=FALSE, fig.cap= 'Figure 3: Missing values in rows', fig.align='center', , fig.height=c(4), fig.width=c(7)}


row.plot <- bands %>%
  mutate(id = row_number()) %>%
  gather(-id, key = "key", value = "val") %>%
  mutate(isna = is.na(val)) %>%
  ggplot(aes(key, id, fill = isna)) +
    geom_raster(alpha=0.8) +
    scale_fill_manual(name = "",
        values = c('steelblue', 'tomato2'),
        labels = c("Present", "Missing")) +
    labs(x = "Variable",
           y = "Row Number", title = "Missing values in rows") +
    coord_flip()

row.plot
```

Figure 3 is confirming the previously stated intuition. The reparation of missing values seems to be concentrated in the very last observations for several variables. This indicates that chunks of consecutive values are missing. This could be due to a problem in recording the observations, or more simply due to the fact that from a certain point in time, the company decided  not to record these specific variables anymore. (*Note. In a real case scenario, the anaysts would have the possibility to ask more information about this subject).

For a total of 13 variables, information is missing starting the 486th observation, which means that data is lost on the respective variables. This raises the question of weather it is useful to keep these observations for the analysis; deleting them would reduce the dataset of 54 rows.

At the same time, keeping them implies having to replace the missing values by an imputation method (knn, mean, median...). Regardless of the method that is chosen for replacing the absent values, this assumes that 54 consecutive observations will share the same value , thus having 10% of the values will be identical. It is strongly believed that this is going to biased the results. Based on this point, it has been decided to delete the last 54 consecutive observations of the dataset, which results in having a  dataset with 485 observations.

```{r delete last obs, message=FALSE, warning=FALSE}
#Delete the last rows that have consecutive missing data
bands1 <- bands[-c(486:540),]
```


The focus is now carried out to the variable **location**, that remains the only variable to still have an important ratio of missing values. Replacing all these values with an imputation method doesn't seem to be the best option, as the proportion of missing is too high compared to the total observations. An alternative has been  decided:  to set these NAs as **UNDEFINED**, this adding a 7th level to the variable. Moreover, location could represent an important factor in the analysis, in the way that problems can be linked to certain locations, thus eliminating it from the models could translate to a loss of information. 


```{r location}
# replace missing values of "location variable" with value "Undefined"
bands1$location = as.character(bands1$location)
bands1$location[is.na(bands1$location)] <- "UNDEFINED"
bands1$location = as.factor(bands1$location)

```


### Imputation of the missing values

The amount of missing values for the rest of the variables is perceived as acceptable and these variables will be imputed by a **k-Nearest-Neighbors** (kNN) method. KNN is an algorithm that is useful for matching a point with its closest k neighbors in a multi-dimensional space. It can be used for data that are continuous, discrete, ordinal and categorical which makes it particularly useful for dealing with all kind of missing data. We have to set a value for **k** and decide to go for the simple approach to set $$k = \sqrt{nrows(dataset)} ={\sqrt{485}} = 22$$

```{r, include=FALSE}
# replace missing values with knn 

n <- nrow(bands1)

k <- sqrt(n)


bands1 <- knnImputation(bands1, k=22)


```



A last check if there are still missing values is performed here below by using the 'anyNA' command, to assure that all the observations have a value.
```{r, include = FALSE}
# check missing value
anyNA(bands1)
```



## Selection of the variables

### First elimination


As the database is complete and has no more missing values, the variable are examined again, only that this time from a different  perspective: variable importance.

There are a lot of attributes that are useless to define if the product presents defects or not. First of all,  variables with only 1 level of factor,such as  **ink_color** and **cylinder_division** can be removed as they won't have any effect on the response variable. Moreover, the variable **job_number** has too many levels (more than 200) and no link with  the response variable. The variables **customer** and **date** can also be deleted as it have no possibilities to affect the class of **band_type**. 

In the case of the variable **cylinder_no**,  further analysis is required. The hight number of levels indicate that this variable is better to be deleted, because it has almost the same number of levels (429) as observations (485) and it doesn't make sense to keep it to define the class 'band_type'. Nevertheless, the cylinder is the most important part of the printing process, as when a problem of banding is detected in the process, the cylinder has to be removed and repaired. This means that it has a direct effect on the response variable. Having a closer look at the variable, it has been noticed that the values for this variable were fallowing a pattern :  "LETTER NUMBER NUMBER NUMBER". Thus, the letter could correspond to a batch number, cylinder type  or even the location where it has been produced, which actually could explain the variable of interest. It has been decided to keep only the letter out of the whole string, for each value of this variable. Doing so, the number of levels drops to 17, which is acceptable in terms of the analysis. (*Note. In a real case scenario, the anaysts would have had the possibility to ask more information about this subject).

```{r delete var}
# Delete variables that are not needed
bands1 <-
  bands1[, which(
    !names(bands1) %in% c(
      "date", # useless
      "ink_color", # only 1 level
      "customer", # don't depend on the printing processus
      'cylinder_division', # only 1 level
      'job_number' # too many levels
    )
  )]

bands1$cylinder_no <- as.factor(substr(bands1$cylinder_no, 0, 1))
```


This first attempt to reduce the number of attribute results in a database containing 35 variables, which seems still high if the company is looking to identify the cause of the defective product. To investigate even further, a method based on statistical features will be used in order to select the most important attributes to keep for the analysis. 


### Boruta test - Second elimination


The Boruta algorithm is useful to define the importance of a variable in a dataset with respect to a response variable. It is built around a Random Forest classification algorithm. 
We can explain how the algorithm works with help of this [website](https://www.r-bloggers.com/feature-selection-with-the-boruta-algorithm/).

"Let’s assume we have a target vector T (the response variable **band_type**) and a bunch of predictors P (all the other variables).

The Boruta algorithm starts by duplicating every variable in P, but instead of making a row-for-row copy, it permutes the order of the values in each column. So, in the copied columns (let’s call them P’), there should be no relationship between the values and the target vector.

Boruta then trains a Random Forest to predict T based on P and P’.

The algorithm then compares the variable importance scores for each variable in P with it’s “shadow” in P’. If the distribution of variable importance is significantly greater in P than it is in P’, then the Boruta algorithm considers that variable significant."

This next part of the analysis takes a closer look at the results after applying the algorithm to the dataset.
```{r message=FALSE, warning=FALSE, include=FALSE}


boruta_test <- Boruta(band_type ~ ., data = bands1, doTrace = 2)


```


```{r plot boruta, echo=FALSE, fig.align='center', fig.cap= 'Figure 4: Importance of the variables', fig.height=c(5), fig.width=c(10)}

plot(boruta_test, xlab = "", xaxt = "n")
lz <- lapply(1:ncol(boruta_test$ImpHistory), function(i)
  boruta_test$ImpHistory[is.finite(boruta_test$ImpHistory[, i]), i])
names(lz) <- colnames(boruta_test$ImpHistory)
Labels <- sort(sapply(lz, median))
axis(
  side = 1,
  las = 2,
  labels = names(Labels),
  at = 1:ncol(boruta_test$ImpHistory),
  cex.axis = 0.7
)
```




```{r datatable boruta, include=FALSE}
table_result <- as.data.frame(boruta_test$finalDecision)
setDT(table_result, keep.rownames = TRUE)[]
colnames(table_result) <- c("Variable", "Decision")

```


```{r echo=FALSE, fig.align='center', fig.cap= 'Figure 5: Importance of the variables'}
datatable(table_result,
  caption = htmltools::tags$caption(style = "caption-side: top; text-align: left;",
                                        "Note. ", htmltools::em("Please select the variables of interest")),
      filter = list(position = 'top',
                    clear = TRUE,
                    plain = FALSE))
```



The results of the Boruta algorithm are shown in Figure 4 where the variables are plotted against their importance value and in Figure 5 where a table was created in which variables are labeled as 'Confirmed', 'Tentative' and 'Rejected' .
The decision is to keep the variables that belong to the first two categories.



```{r message=FALSE, warning=FALSE, include = FALSE}
variables_to_keep <- table_result %>%
  filter(table_result$Decision == 'Confirmed' |
           table_result$Decision == 'Tentative')



band_typeVar <- data.frame('band_type', 'Confirmed')
names(band_typeVar) <- c('Variable', 'Decision')

variables_to_keep <- rbind(variables_to_keep, band_typeVar)

names_variables_to_keep <- variables_to_keep$Variable

names_variables_to_keep

bands1 <-
  bands1[, which(
    names(bands1) %in% names_variables_to_keep
  )]
```

A first look at the database after all the modifications that have been done untill now.

```{r}

datatable(bands1,
  caption = htmltools::tags$caption(style = "caption-side: top; text-align: left;",
                                        "Note. ", htmltools::em("Please select the variables of interest")),
      filter = list(position = 'top',
                    clear = TRUE,
                    plain = FALSE))

```




## Outliers

After a close verification of the boxplots and the summary for each variable, it has been concluded that there are no error measurements and neither error entries in the data. Please refer to the Annexe1, to see the voxplots.

```{r, message=FALSE, warning=FALSE, include=FALSE}

numeric_column <- names(select_if(bands1, is.numeric))

for (i in numeric_column) {

  print(summary(bands1[i]))
  print (boxplot(bands1[i])$out)

}

```
Here below a short summary of the numerical variables.

```{r}

quanti<-1:length(names(select_if(bands1, is.numeric)))
mat<-matrix(0, length(quanti), 7)
colnames(mat)<-c("Min.", "1st Qu.", "Median", "Mean", "Sd", "3rd Qu.", "Max.")
rownames(mat)<-names(select_if(bands1, is.numeric))
mat[,1]<-apply(bands1[,names(select_if(bands1, is.numeric))], 2, min)
mat[,2]<-apply(bands1[,names(select_if(bands1, is.numeric))], 2, quantile, 0.25)
mat[,3]<-apply(bands1[,names(select_if(bands1, is.numeric))], 2, median)
mat[,4]<-apply(bands1[,names(select_if(bands1, is.numeric))], 2, mean)
mat[,5]<-apply(bands1[,names(select_if(bands1, is.numeric))], 2, sd)
mat[,6]<-apply(bands1[,names(select_if(bands1, is.numeric))], 2, quantile, 0.75)
mat[,7]<-apply(bands1[,names(select_if(bands1, is.numeric))], 2, max)

round(mat,1)%>%
kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover","condensed","bordered"),full_width = F, position = "center")%>%
  footnote(symbol = strwrap("Summary of the data"))


```


# Exploratory Data Analysis

Th Variables have been plotted against the variables of interest, 'band_type'.

```{r fig.height= 12}

plot_boxplot(bands1, by= 'band_type',  ncol = 2, title = "Side-by-side boxplots")

```

```{r, fig.width = 10, fig.height = 20}


bands1%>%
  explore_all(target = band_type)
```





# Fitting the models


## test and train sets

In order to construct algorithms that can learn from the data that is provided and to be able to make predictions on new instances, the cleaned database was split in two sunsets: a train set and a test set. The model is first fitted on the train set and the predictions would be done on the test set.

The general method to split the whole dataset is to extract 75% of the dataset, which will constitute the train set, and the remaining 25% will represent the test set. The **caret** package has a very useful function (**createDataPartition**) to do this task and allows to keep the same repartition of each classes in the two sets. This function has been used throughout all the models.


All the models have been created using the 'caret' package. The **trControl** option of the **caret** package,  allows to include  computational nuances of the train function. For this  model a **repeatedcv** option has been chosen as the method of resampling.  This implies that the training dataset will be divided randomly into 10 parts. Each chunck will serve in turns as a test set for the model that has been buid using the other nine datasets (which sereved as a trin set). This **repeatedcv** option leads to robust models and for this reason the method will be also present for the other models. 

Other option that will be used in all the models that are presented in this paper is the **sampling = 'down'** option of **trControl**. This feature will correct the problem of unbalanced classes in the dataset. This is an essential aspect as the class **BAND** has fewer instances than the class **NOBANDS**. The scope of the models is to predict with the most higher accuracy the observations classified as **BAND**. 

```{r message=FALSE, warning=FALSE}


# Create the training and test datasets
set.seed(100)

# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(bands1$band_type, p=0.75, list=FALSE)

# Step 2: Create the training  dataset
trainData <- bands1[trainRowNumbers,]

# Step 3: Create the test dataset
testData <- bands1[-trainRowNumbers,]

# the proportion of the classes (BAND and NOBAND) stay the same in the train and test set (65 and 35% respectively)

table(trainData$band_type)
table(testData$band_type)
```



## Random Forest



The Random Forest algorithm is an ensemble learning technique. The algorithm builds multiple decision trees that are trained on  different subsets of data of the training set. The predictions are then averaged, in order to create the final output.


The Random Forest model gives the possibility to view the variables of the dataset that are perceived as important for the selected model. This information could be used to build new models that are using only the varriables that have been labeled as important, thus reducing the complexity of the model.


```{r message=FALSE, warning=FALSE, include= FALSE}
integ_mod_1 <- data_integrity_model(data = bands1, model_name = "randomForest")
integ_mod_1$data_ok


# Any errors ?
integ_mod_1

set.seed(12345)
model_rf <- caret::train(band_type ~ .,
                         data=trainData,
                         method="rf",
                         preProcess=NULL,
                         trControl=trainControl(method="repeatedcv", number=10,
                                                repeats=10, verboseIter=FALSE, sampling = "down"))
weather.pred.1 <- predict(model_rf, testData)


```


```{r}
confusionMatrix(weather.pred.1, testData$band_type, positive = 'NOBAND')
```

The first model gives an accuracy of 77%, with sensitivity reaching 76% and specifity being 79%. This first reasults are encouriging, as the class that interests most is "BAND" and the model is able to predict 34 times correct out of a total of 43 instances. 



Interpretation
- first model so no points of comparison
- accuracy of 77% 
- better specificity than sensitivity -> predict better Band than Noband(?)
- results seems acceptable, waitng to compare with others models

```{r}

varimp_rf <- varImp(model_rf)
plot(varimp_rf, main = "Variable Importance with Random Forest")

```
The plot  lists the variables  in the order of importance. According to the model, the variables **press_speed**, **viscosity**, **blade_pressure**, **press**, **proof_cut** are the top five attributes that explain the variable of interest.



## Nearest Neighbour Classification 
The next model used in the analysis in the Nearest Neighbour Classifier, 

```{r message=FALSE, warning=FALSE}

trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3, sampling = 'down')
set.seed(3333)
knn_fit <- train(band_type ~., data = trainData, method = "knn",
 trControl=trctrl,
 preProcess = c("center", "scale"),
 tuneLength = 10)

knn_fit

test_pred_knn <- predict(knn_fit, newdata = testData)


```

```{r}
confusionMatrix(test_pred_knn, testData$band_type, positive = 'NOBAND')
```

The second  model provided an accuracy of 69%, lower than the one resulted from using the Random Forest algoristh. The sensitivity level and Specificity are 67% and 72% respectively. The two measures have also decresed in contrast with the previous model, but the poucentage for Specificity is still above 70% and given that the company is most interested in predicting well the **BANDS* instances, this model can still be considered satisfying. 

interpretation
- Accuracy was used to select the optimal model using the largest value.
The final value used for the model was k = 9.
- accuracy 69% 
- below the RF 
- 


## Logistic Regression


The third model that has been considered is the Logistic regression. As in the case of the other models, the model has been buid using caret.


Logistic regression is binomial regression model widely used in machine learning when the dependent variable is categorical.


```{r message=FALSE, warning=FALSE}
# logistic regression

fitControl <- trainControl(method = "repeatedcv", sampling = 'down', )
set.seed(9090)
model_lr <-
  train(band_type ~ .,
        data = trainData,
        method = "glm",
        trControl = fitControl)

print(model_lr$finalModel)

pred <- predict(model_lr, newdata = testData)

print(table(pred))

mat <-
  confusionMatrix(data = pred,
                  reference = testData$band_type,
                  positive = "NOBAND")
print(mat)

# the results are very poor, so we will focus on other model, because this one would be difficult to improve

```

In the case of Logistic regression, the an accuracy has a level of 74%. The Sensitivity level and Specificity are 76% and 69% respectively. It is interesting as this model is better at predicting the **NOBANDS** rather thand **BANDS**, as opposed to the two previous models. 

For the moment this model is the least important as the Specificity is the lowest, and the prior models offered better results for the prediction of this class.

## Support Vector Machine 1

Support Vector Machines are supervised learning models, which task is to analyze data used for classification and regression analysis with help of associated learning algorithm.



```{r}
# SVM1

fitControl.svm <- trainControl(method = "cv", number = 5, sampling = 'down')

set.seed(838)
model_svm <- train(
  band_type ~ .,
  data =
    trainData,
  method = "svmLinear",
  trControl = fitControl.svm,
  tuneGrid = data.frame(C = 10)
)
print(model_svm)

varimp_svm <- varImp(model_svm)
varimp_svm
plot(varimp_svm, main = "Variable Importance with SVM")


print(confusionMatrix(
  data = predict(model_svm, newdata =
                   testData),
  reference = testData$band_type,
  positive = "NOBAND"
))




```

## Support Vector Machine 2

- explain how it works
- fit the model
- results

```{r}
#SVM2


model_svm2 <- train(band_type ~ .,
                          data = trainData, 
                          method = "svmRadialCost",
                          preProcess = "range",
                          trace = FALSE,
                          trControl = trainControl(method = "repeatedcv", number = 10, repeats = 10, 
                                                  verboseIter = FALSE, sampling = 'down'))
model_svm2

plot(model_svm2)

print(confusionMatrix(
  data = predict(model_svm2, newdata =
                   testData),
  reference = testData$band_type,
  positive = "NOBAND"
))




```


## Neural Network


```{r neural network model}

set.seed(890)
model_nnet <- caret::train(band_type ~ ., data = trainData, method = "nnet", preProcess = "range", 
    trace = FALSE, trControl = trainControl(method = "cv", sampling = 'down'))

model_nnet

varimp_nnet <- varImp(model_nnet)
varimp_nnet
plot(varimp_nnet, main = "Variable Importance with Nnet")

results1 <- predict(model_nnet, newdata = testData)
results1

conf1 <-
  confusionMatrix(results1, testData$band_type, positive = 'NOBAND')
conf1



```

## Classification Tree

- explain how it works
- fit the model
- pruning...
- results

```{r}

library(rpart)
library(rpart.plot)
# Build the model
set.seed(123)
model_cart <- rpart(band_type ~., data = trainData, method = "class")  
model_cart
summary(model_cart)

# pruning
par(pty = "s")
with(model_cart, plot(cptable[, 3], xlab = "Tree Number", ylab = "Resubstitution Error (R)", 
    type = "b"))

# Plot the trees
rpart.plot(model_cart, box.palette = "RdBu", nn = FALSE)

printcp(model_cart)


#pruning
cart_prune <- prune(model_cart, cp = 0.01)



cart.pred<-predict(model_cart, testData, type="class")
cart.pred

table(true=testData$band_type, pred = cart.pred)

CrossTable(x=testData$band_type, y=cart.pred, prop.chisq=FALSE)

confusionMatrix(cart.pred, testData$band_type, positive = 'NOBAND')

predict_cart <- predict(model_cart, newdata = testData)


```





```{r,  echo = FALSE, warning = FALSE, message = FALSE, out.width = "700px"}

knitr::include_graphics("C:/Users/claud/Desktop/Ana/HEC/3rd semester/Projects in Data Analytics/project/dadm2019/pictures/attributes1.PNG")

```

```{r,  echo = FALSE, warning = FALSE, message = FALSE, out.width = "700px"}

knitr::include_graphics("C:/Users/claud/Desktop/Ana/HEC/3rd semester/Projects in Data Analytics/project/dadm2019/pictures/attributes2.PNG")

```




```{r}
varimp_rf <- varImp(model_rf)
varimp_rf
plot(varimp_rf, main = "Variable Importance with SVM")

bands.rf <- bands1[, which(
    names(bands1) %in% c(
      "blade_pressure", 
      "press", 
      "press_speed", 
      'ink_pct', 
      'viscosity',
      'ink_temperature',
      'proof_cut',
      'band_type',
      "humidity",
      'solvent_pct',
      'varnish_pct'
    )
  )]
```

- here we plot the variables with the most important according to Random Forest
-be useful for further analysis
- we will compute simpler models 

```{r}
# Random forest with only important variable

integ_mod_1 <- data_integrity_model(data = bands.rf, model_name = "randomForest")
integ_mod_1$data_ok




# Any errors ?
integ_mod_1


# Create the training and test datasets
set.seed(100)

# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(bands.rf$band_type, p=0.75, list=FALSE)

# Step 2: Create the training  dataset
trainData.rf <- bands.rf[trainRowNumbers,]

# Step 3: Create the test dataset
testData.rf <- bands.rf[-trainRowNumbers,]

# the proportion of the classes (BAND and NOBAND) stay the same in the train and test set (65 and 35% respectively)

set.seed(12345)
model_rf.imp <- caret::train(band_type ~ .,
                         data=trainData.rf,
                         method="rf",
                         preProcess=NULL,
                         trControl=trainControl(method="repeatedcv", number=10,
                                                repeats=10, verboseIter=FALSE, sampling = "down"))
weather.pred.1 <- predict(model_rf.imp, testData.rf)
confusionMatrix(weather.pred.1, testData.rf$band_type, positive = 'NOBAND')


```






